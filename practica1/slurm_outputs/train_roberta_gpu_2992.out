Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[W327 11:01:44.905295410 collection.cpp:992] Warning: Failed to recover relationship between all profiler and kineto events: 12405 vs. 0  reassociated. (function reassociate)
Epoch 1, Loss: 0.6934
Epoch 2, Loss: 0.6799
Epoch 3, Loss: 0.7024
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        1.535s        26.17%        1.535s       7.008ms           0 b           0 b           0 b           0 b           219  
void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        1.483s        25.30%        1.483s       6.868ms           0 b           0 b           0 b           0 b           216  
void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        1.338s        22.82%        1.338s       6.027ms           0 b           0 b           0 b           0 b           222  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     103.111ms         1.76%     103.111ms     116.906us           0 b           0 b           0 b           0 b           882  
void (anonymous namespace)::softmax_warp_backward<fl...         0.00%       0.000us         0.00%       0.000us       0.000us      90.630ms         1.55%      90.630ms       2.518ms           0 b           0 b           0 b           0 b            36  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      86.747ms         1.48%      86.747ms       2.410ms           0 b           0 b           0 b           0 b            36  
void at::native::(anonymous namespace)::fused_dropou...         0.00%       0.000us         0.00%       0.000us       0.000us      73.483ms         1.25%      73.483ms       1.884ms           0 b           0 b           0 b           0 b            39  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      71.856ms         1.23%      71.856ms       1.996ms           0 b           0 b           0 b           0 b            36  
                                   volta_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us      69.659ms         1.19%      69.659ms       1.935ms           0 b           0 b           0 b           0 b            36  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      67.234ms         1.15%      67.234ms       1.601ms           0 b           0 b           0 b           0 b            42  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 6.534s
Self CUDA time total: 5.864s

Training completed.
Training time without initializations: 8.60 seconds.
